{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "950b2909-18bb-42f9-bad0-47dae9b6760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c072d950-fb92-4e7d-859b-b8d6cba99179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CloudDetect.util import read_off, visualise\n",
    "from CloudDetect.transform import PointCloudSample, Normalise, Tensor\n",
    "from CloudDetect.models import PointNet\n",
    "from CloudDetect.dataset import CloudDataset, assign_val_indices\n",
    "from CloudDetect.util import get_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc162eca-cb06-4cff-9a37-30e57f145827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eea590bb-ad5f-43ea-9da9-77a34e6fd954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "metadata = pd.read_csv('data/metadata_modelnet40.csv')\n",
    "all_data_files = glob.glob('**/*.off',recursive=True)\n",
    "all_data_files = set(['/'.join(x.split('/')[2:]) for x in all_data_files])\n",
    "metadata = metadata[metadata['object_path'].map(lambda x: x in all_data_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b10600c-7a3c-40df-8359-8742e48129ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00e12d78-a3bf-41e3-aac3-ac52fa1a02d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(n_sample, norm_how):\n",
    "    composition = [\n",
    "            PointCloudSample(n_sample),\n",
    "            Normalise(norm_how),\n",
    "            Tensor()\n",
    "    ]\n",
    "    return transforms.Compose(composition)\n",
    "\n",
    "def generate_class_mapper(metadata):\n",
    "    class_mapper = {x:i for i,x in enumerate(metadata['class'].unique())}\n",
    "    return class_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "325e00ba-3a5b-460b-afd6-184d865e58a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_train = metadata[metadata['split'] == 'train']\n",
    "class_mapper = generate_class_mapper(metadata_train)\n",
    "metadata_test = metadata[metadata['split'] == 'test']\n",
    "metadata_train = metadata_train.reset_index(drop = True)\n",
    "metadata_test = metadata_test.reset_index(drop = True)\n",
    "assign_val_indices(metadata_train, 'class', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "070f4fc5-0865-4f38-93dc-d129a5e898e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0b2fe97-c890-4f6f-a412-fad069ffb484",
   "metadata": {},
   "outputs": [],
   "source": [
    "class training(object):\n",
    "    def __init__(self, \n",
    "                 loss_fn, \n",
    "                 optimiser, \n",
    "                 model,\n",
    "                train_loader,\n",
    "                val_loader):\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimiser = optimiser\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "    def load_data(self, data):\n",
    "        X,y = data['data'], data['category']\n",
    "        X = torch.transpose(X.float(),1,2)\n",
    "        return X, y\n",
    "    \n",
    "    def perform_optimisation(self,X, y):\n",
    "        self.optimiser.zero_grad()\n",
    "        model_output = self.model(X)\n",
    "        loss = self.loss_fn(model_output, y)\n",
    "        loss.backward()\n",
    "        optimiser.step() \n",
    "        return loss\n",
    "    \n",
    "    def reporting(self, batch_print, running_loss):\n",
    "        last_loss = running_loss / batch_print # average loss per batch\n",
    "        return last_loss\n",
    "\n",
    "    def run_epoch(self):\n",
    "        running_loss = 0\n",
    "        last_loss = 0\n",
    "        epoch_index = 0\n",
    "        tracker = []\n",
    "        batch_print = 1\n",
    "        last_loss = -999\n",
    "        val_loss = -999\n",
    "        pbar = tqdm(enumerate(self.train_loader), total=len(self.train_loader))\n",
    "        for i,x in pbar:\n",
    "            pbar.set_description(f\"BATCH TRAIN LOSS {round(last_loss,5)} - VAL LOSS {round(val_loss,5)}\")\n",
    "            # load data\n",
    "            X,y = self.load_data(x)\n",
    "            loss = self.perform_optimisation(X, y)\n",
    "            running_loss += loss.item()\n",
    "            if i % batch_print == 0 and i!=0:\n",
    "                last_loss = self.reporting(batch_print, running_loss)\n",
    "                running_loss = 0\n",
    "        \n",
    "                #validate\n",
    "                all_true_output = []\n",
    "                all_model_output = []\n",
    "                for i,x in enumerate(self.val_loader):\n",
    "                    # load data\n",
    "                    X,y = self.load_data(x)\n",
    "                    model_output = self.model(X)\n",
    "                    all_true_output.append(y)\n",
    "                    all_model_output.append(model_output)\n",
    "        \n",
    "                all_true = torch.concat(all_true_output)\n",
    "                all_model = torch.concat(all_model_output)\n",
    "                loss = loss_fn(all_model, all_true)\n",
    "                val_loss = loss.item()\n",
    "        \n",
    "        \n",
    "        #validate\n",
    "        all_true_output = []\n",
    "        all_model_output = []\n",
    "        for i,x in tqdm(enumerate(self.val_loader), total=len(self.val_loader)):\n",
    "            # load data\n",
    "            X,y = self.load_data(x)\n",
    "            model_output = self.model(X)\n",
    "            all_true_output.append(y)\n",
    "            all_model_output.append(model_output)\n",
    "        \n",
    "        all_true = torch.concat(all_true_output)\n",
    "        all_model = torch.concat(all_model_output)\n",
    "        loss = self.loss_fn(all_model, all_true)\n",
    "        print(loss)\n",
    "        classification_output = torch.argmax(torch.exp(all_model),axis = 1)\n",
    "        results = get_metrics(all_true, classification_output)\n",
    "        print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "436c0a08-1d31-4ccc-959b-95267f67672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelling\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "model = PointNet(n_point = 1024, classes = len(class_mapper), segment = False)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "085a3c49-180a-4e5a-b28a-f07a27f43d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx = 1\n",
    "metadata_train_val = metadata_train[metadata_train['kfold'] == val_idx]\n",
    "metadata_train_train = metadata_train[metadata_train['kfold'] != val_idx]\n",
    "\n",
    "# jsut for quick training\n",
    "metadata_train_train = metadata_train_train.sample(100)\n",
    "metadata_train_val = metadata_train_val.sample(100)\n",
    "\n",
    "# dataset\n",
    "ROOT = 'data/ModelNet40/'\n",
    "preprocessor = preprocessing(1024, 'max')\n",
    "class_mapper = generate_class_mapper(metadata)\n",
    "cloud_train_dataset = CloudDataset(metadata_train_train, preprocessor, ROOT, class_mapper)\n",
    "cloud_val_dataset = CloudDataset(metadata_train_val, preprocessor, ROOT, class_mapper)\n",
    "CloudDataTrainLoader = DataLoader(cloud_train_dataset, batch_size=32, shuffle=True)\n",
    "CloudDataValLoader = DataLoader(cloud_val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "873ff64c-5408-431f-a096-dfe8feb582c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_object = training(loss_fn, optimiser, model, CloudDataTrainLoader, CloudDataValLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b22d466f-5f56-437b-9c15-44b2eeb86e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BATCH TRAIN LOSS 3.77654 - VAL LOSS 3.69732: 100%|█| 4/4 [00:42<00:00, 10.71s/it\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:11<00:00,  2.77s/it]\n",
      "/opt/miniconda3/envs/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2458: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.6603, grad_fn=<NllLossBackward0>)\n",
      "{'f1_score': 0.041588050314465407, 'precision': 0.052210365853658534, 'recall': 0.05729166666666667, 'balanced_acc': 0.06547619047619048}\n"
     ]
    }
   ],
   "source": [
    "train_object.run_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c917e45b-3fea-41b7-b71a-8d3763561972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
